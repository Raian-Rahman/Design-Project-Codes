{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Using Pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmgyDeVfHo7ETDzzXQ1sXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raian-Rahman/Design-Project-Codes/blob/main/Transformer_Using_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1JjdF0Mn1O6"
      },
      "source": [
        "### Importing the libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72wxTmzqnwtr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import unsqueeze\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        \"\"\"\n",
        "        We are going to split the embed size between these heads. If we have 256 size embedding and 8 heads then we will have 32 embed size for each embedding\n",
        "        \"\"\"\n",
        "        super(SelfAttention,self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        \n",
        "        assert self.embed_size%self.heads == 0 , \"To make sure that embed size is properly divisible by heads\"\n",
        "        \n",
        "        self.head_dim = embed_size//heads\n",
        "\n",
        "        \"\"\"\n",
        "        Now we are defining the Query value and key vectors as Linear layers. \n",
        "        We are setting bias = False, because we dont need that\n",
        "        \"\"\"\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, queries, mask):\n",
        "        N = queries.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
        "\n",
        "        \"\"\"\n",
        "        split embedding into self.heads pieces\n",
        "        \"\"\"\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        #step 1: multiply query and key\n",
        "        \n",
        "        # queries shape : (N, query_len, heads, heads_dim)\n",
        "        # keys shape: (N, key_len, heads, heads_dim)\n",
        "        # energy shape: (N, heads,query_len = target source sentence, key_len = source sentence)\n",
        "        \"\"\"\n",
        "        As we have a batch matrix multiplier einsum is quite handy for it \n",
        "        \"\"\"\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries,keys]) #it is used for matrix multiplication where we have several other dimensions \n",
        "        \n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask==0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy/(self.embed_size**(1/2)),dim=3)\n",
        "\n",
        "\n",
        "        #attention shape: (N,heads, query_len, key_len)\n",
        "        #value shape: (N, value_len, heads, heads_dim)\n",
        "        #out shape: (N, Query_len, heads, heads_dim)\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention,values])\n",
        "\n",
        "\n",
        "        #concatanation part \n",
        "        out  = out.reshape(N,query_len, self.heads*self.head_dim)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxSmB2z_vhF"
      },
      "source": [
        "### Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUzt6d5_uI0l"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    embedding -> multiheaded_attention -> add&norm -> feed forward -> add&norm \n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock,self).__init__()\n",
        "        self.attention = SelfAttention(embed_size=embed_size, heads = heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size) #layernorm and batchnorm are almost similar...but layer norm has more computation\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion*embed_size,embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "\n",
        "        \"\"\"\n",
        "        we needed a skip connection. query is for the skip connection\n",
        "        \"\"\"\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward+x))\n",
        "        print(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7o7oW8bBLh9"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
        "        \"\"\"\n",
        "        Encoder block takes a lot of parameters due to hyperparameter. The parameters are explained below:\n",
        "        ---------------------------------------------------------------------------------------------------\n",
        "        src_vocab_size = size of source vocabulary \n",
        "        embed_size  = dimension of embedding \n",
        "        num_layers = number of transformer layer in encoder\n",
        "        heads = number of heads in multiheads \n",
        "        device = the device on which we want to train\n",
        "        forward_expansion  = the ratio by which we want to expand the size\n",
        "        dropout  = dropout probability\n",
        "        max_length = max sentence length. \n",
        "        maximum length of string to ensure positional embedding which is requeired for ensuring we have attention. \n",
        "        What transformer does is we wnat to ensure that some sort of sequence is maintained even is the layer does not have any recurrent unit. It helps the transformer for ensuring parallelization\n",
        "        \"\"\"\n",
        "        super(Encoder,self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [ \n",
        "              TransformerBlock(embed_size=embed_size, heads=heads, dropout=dropout, forward_expansion=forward_expansion)  for _ in range(num_layers)\n",
        "            ] \n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        \n",
        "        N, seq_length = x.shape\n",
        "\n",
        "        positions = torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n",
        "\n",
        "        out = self.dropout(self.word_embedding(x)+self.positional_embedding(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out,out,out,mask)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LutflV_FRrR"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "        super(DecoderBlock,self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, value, key, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        Decoder block takes a lot of parameters. The parameters are explained below:\n",
        "        ----------------------------------------------------------------------------\n",
        "        x : input \n",
        "        value, key : for self_attention\n",
        "        src_mask: source mask. Although it is optional still we need it. For example, let we have more than one example in the input. In those cases src_mask is needed to make all the sentences equal also we dont need to to extra computations for the masks that are padded\n",
        "        trg_mask: trg_mask is required to make sure that everything works fine\n",
        "        \"\"\"\n",
        "        attention = self.attention(x,x,x,trg_mask)\n",
        "        query = self.dropout(self.norm(attention+x))\n",
        "        out = self.transformer_block(value, key, query, src_mask)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox-_ZuWpYQwC"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.positional_embedding = nn.Embedding(max_length,embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderBlock(embed_size, heads, forward_expansion, dropout, device) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n",
        "        x = self.dropout(self.word_embedding(x)+self.positional_embedding(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkWj30n7ejgp"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size = 256, num_layers = 6, forward_expansion = 4, heads = 8, dropout = 0, device = \"cuda\", max_length = 100):\n",
        "        super(Transformer,self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size,embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
        "        self.decoder = Decoder(trg_vocab_size,embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def make_src_mask(self,src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self,trg):\n",
        "        N, trg_len = trg.shape\n",
        "\n",
        "        trg_mask = torch.tril(torch.ones((trg_len,trg_len))).expand(N,1,trg_len, trg_len)\n",
        "\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx1y1NMnhORX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e75d5b06-efdc-48fb-fcae-8e54b55803a9"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    x = torch.tensor([[1,5,6,4,3,9,5,2,0],[1,8,7,3,4,5,6,7,2]]).to(device)\n",
        "    trg = torch.tensor([[1,7,4,3,5,9,2,0],[1,8,7,3,4,5,6,2]]).to(device)\n",
        "\n",
        "    src_pad_idx = 0\n",
        "\n",
        "    trg_pad_idx = 0\n",
        "    src_vocab_size = 10\n",
        "    trg_vocab_Size = 10\n",
        "\n",
        "    model = Transformer(src_vocab_size, trg_vocab_Size, src_pad_idx, trg_pad_idx).to(device)\n",
        "\n",
        "    out = model(x,trg[:,:-1])\n",
        "    print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-4.3455e-01, -3.5848e-01, -1.7180e-01,  ..., -5.9025e-01,\n",
            "           6.5345e-01, -8.4166e-01],\n",
            "         [-1.1427e+00,  1.1005e+00, -4.9799e-01,  ...,  1.7247e+00,\n",
            "          -9.7928e-01, -2.1180e-01],\n",
            "         [-4.3570e+00,  1.2240e+00,  1.8246e-02,  ..., -6.0344e-01,\n",
            "          -2.1117e-01, -1.6770e-01],\n",
            "         ...,\n",
            "         [-1.3541e+00, -1.1390e-01, -7.2088e-02,  ...,  1.4460e+00,\n",
            "           4.9995e-02,  5.4499e-01],\n",
            "         [-2.8333e-01, -8.6744e-01,  2.5015e-02,  ..., -6.0208e-02,\n",
            "          -4.4728e-01,  1.2987e+00],\n",
            "         [-1.8219e+00,  5.7185e-01,  1.5677e+00,  ..., -7.2762e-01,\n",
            "          -2.7209e-01,  4.4812e-03]],\n",
            "\n",
            "        [[-2.5805e-01, -5.1294e-01, -1.0748e-01,  ..., -6.3579e-01,\n",
            "           6.4053e-01, -7.9476e-01],\n",
            "         [ 3.4465e-01,  3.0850e-01, -6.2003e-01,  ...,  8.0095e-01,\n",
            "           4.3285e-01, -1.8668e-01],\n",
            "         [-1.3339e+00,  8.7999e-01,  5.0519e-01,  ...,  1.8432e-01,\n",
            "          -3.1823e-01,  1.6017e-01],\n",
            "         ...,\n",
            "         [-2.4430e+00, -7.2436e-01, -4.8024e-01,  ..., -2.9667e-01,\n",
            "           5.7626e-01,  2.0823e-03],\n",
            "         [ 2.2336e-01, -4.0720e-01,  2.0113e-01,  ...,  1.7275e-01,\n",
            "          -6.3510e-01,  6.8051e-01],\n",
            "         [-1.1326e-01, -4.6874e-01, -2.6607e-01,  ..., -9.4948e-01,\n",
            "           2.0026e-01, -6.9253e-02]]], device='cuda:0',\n",
            "       grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[-0.3913, -0.1148, -0.4352,  ..., -0.6583,  0.0857, -1.0611],\n",
            "         [-1.4038,  1.6378, -1.1080,  ...,  1.7922, -1.0300, -0.1628],\n",
            "         [-4.4108,  1.6582, -0.1065,  ..., -0.5330, -0.4113,  0.1222],\n",
            "         ...,\n",
            "         [-1.6678,  0.3719, -0.4459,  ...,  1.5146, -0.4852,  0.8484],\n",
            "         [-0.5854, -0.1476,  0.0428,  ..., -0.1189, -0.6165,  1.3146],\n",
            "         [-1.6874,  0.9033,  1.2529,  ..., -0.9568, -0.6574, -0.0375]],\n",
            "\n",
            "        [[-0.2256, -0.4363, -0.4340,  ..., -0.7617,  0.1198, -1.0349],\n",
            "         [ 0.1233,  0.6635, -1.0910,  ...,  0.6670,  0.3822, -0.3258],\n",
            "         [-1.4936,  1.1983,  0.0355,  ..., -0.0593, -0.5216,  0.3144],\n",
            "         ...,\n",
            "         [-2.5755, -0.5356, -0.5205,  ..., -0.1499,  0.1545,  0.0058],\n",
            "         [-0.1758,  0.0797, -0.1028,  ...,  0.0917, -0.8232,  0.7636],\n",
            "         [-0.3259, -0.0628, -0.3108,  ..., -1.1250, -0.2131, -0.1299]]],\n",
            "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[-4.3696e-01,  5.6491e-02, -2.8996e-01,  ..., -6.7695e-01,\n",
            "          -3.2977e-01, -7.1515e-01],\n",
            "         [-1.4806e+00,  1.7856e+00, -1.1900e+00,  ...,  1.5941e+00,\n",
            "          -1.0422e+00, -5.8627e-02],\n",
            "         [-4.4986e+00,  1.4208e+00, -5.1943e-01,  ..., -8.5386e-01,\n",
            "          -3.3727e-01,  3.3038e-01],\n",
            "         ...,\n",
            "         [-1.7396e+00,  5.7620e-01, -2.9726e-01,  ...,  1.2206e+00,\n",
            "          -6.9236e-01,  9.1912e-01],\n",
            "         [-7.2524e-01, -1.8397e-02,  1.0493e-01,  ..., -4.3114e-01,\n",
            "          -3.9361e-01,  1.5104e+00],\n",
            "         [-1.7667e+00,  9.0998e-01,  9.8758e-01,  ..., -1.2732e+00,\n",
            "          -5.7193e-01,  2.2810e-01]],\n",
            "\n",
            "        [[-3.7273e-01, -3.1394e-01, -1.9658e-01,  ..., -5.4997e-01,\n",
            "          -4.9766e-01, -8.6126e-01],\n",
            "         [ 1.0870e-02,  8.4251e-01, -1.1001e+00,  ...,  5.3804e-01,\n",
            "           4.5146e-02, -3.8195e-01],\n",
            "         [-1.4959e+00,  9.8792e-01, -6.2432e-02,  ..., -3.4581e-01,\n",
            "          -4.4174e-01,  5.1357e-01],\n",
            "         ...,\n",
            "         [-2.6850e+00, -5.5638e-01, -6.8530e-01,  ..., -2.2947e-02,\n",
            "          -2.5425e-01, -2.7262e-01],\n",
            "         [ 6.5073e-02,  1.9542e-01,  1.3580e-02,  ...,  7.7115e-02,\n",
            "          -8.1966e-01,  9.1767e-01],\n",
            "         [-3.6536e-01, -2.2652e-02, -7.5641e-02,  ..., -1.0594e+00,\n",
            "          -4.3571e-01, -3.0373e-03]]], device='cuda:0',\n",
            "       grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[-0.3399, -0.4601,  0.1997,  ..., -0.2025, -0.4003, -0.2676],\n",
            "         [-1.4031,  1.4204, -0.9593,  ...,  1.9622, -1.0996, -0.1961],\n",
            "         [-4.3938,  0.9351, -0.2055,  ..., -0.6929, -0.5198,  0.5655],\n",
            "         ...,\n",
            "         [-1.6228,  0.3573,  0.1269,  ...,  1.7069, -0.7211,  0.9347],\n",
            "         [-0.5947, -0.0700,  0.3772,  ...,  0.0424, -0.4505,  1.7985],\n",
            "         [-1.6660,  0.7181,  0.9843,  ..., -0.7397, -0.3822,  0.3879]],\n",
            "\n",
            "        [[-0.2012, -0.8525,  0.2136,  ..., -0.2294, -0.4567, -0.3224],\n",
            "         [-0.0746,  0.2314, -1.1761,  ...,  0.7789, -0.0775, -0.4319],\n",
            "         [-1.7949,  0.4585, -0.1444,  ..., -0.2293, -0.4130,  0.9122],\n",
            "         ...,\n",
            "         [-2.3812, -0.8251, -0.0563,  ...,  0.4369, -0.2913, -0.0371],\n",
            "         [ 0.0243, -0.1010,  0.3615,  ...,  0.2313, -0.6555,  1.3086],\n",
            "         [-0.1445, -0.0956,  0.0355,  ..., -0.7079, -0.3640,  0.3014]]],\n",
            "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[-3.2200e-01, -8.2941e-01,  4.2942e-01,  ..., -2.7637e-01,\n",
            "          -1.4541e-01, -5.5988e-01],\n",
            "         [-1.6156e+00,  9.0780e-01, -9.3740e-01,  ...,  2.2486e+00,\n",
            "          -9.0138e-01, -5.0897e-01],\n",
            "         [-4.4016e+00,  7.7790e-01, -1.6484e-01,  ..., -4.0856e-01,\n",
            "          -1.7931e-01,  6.3783e-02],\n",
            "         ...,\n",
            "         [-1.6158e+00, -9.3411e-02,  1.8699e-01,  ...,  1.9055e+00,\n",
            "          -3.9495e-01,  5.6785e-01],\n",
            "         [-8.2571e-01, -4.6718e-01,  5.0498e-01,  ...,  5.5876e-01,\n",
            "          -2.8385e-02,  1.3392e+00],\n",
            "         [-1.7505e+00,  3.8583e-01,  1.0436e+00,  ..., -8.7082e-01,\n",
            "          -1.9177e-01,  2.9710e-01]],\n",
            "\n",
            "        [[-1.4084e-01, -1.1498e+00,  2.9652e-01,  ..., -4.2680e-01,\n",
            "          -3.1473e-01, -5.7758e-01],\n",
            "         [-4.0322e-02, -1.9234e-01, -1.2235e+00,  ...,  9.9110e-01,\n",
            "          -6.5928e-02, -7.8024e-01],\n",
            "         [-2.1963e+00,  1.5257e-01, -1.4941e-01,  ...,  1.5672e-03,\n",
            "           1.8611e-01,  3.5259e-01],\n",
            "         ...,\n",
            "         [-2.3553e+00, -1.1070e+00, -2.0719e-01,  ...,  3.2361e-01,\n",
            "          -1.4351e-01, -3.2729e-01],\n",
            "         [-5.5515e-01, -6.1837e-01,  2.0059e-01,  ...,  4.7006e-01,\n",
            "          -3.7211e-01,  8.5069e-01],\n",
            "         [-3.0626e-01, -5.2303e-01,  1.3688e-01,  ..., -9.0194e-01,\n",
            "          -2.2277e-01,  1.6056e-01]]], device='cuda:0',\n",
            "       grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[-0.5155, -1.0167,  0.3453,  ..., -0.5776, -0.0924, -0.1169],\n",
            "         [-1.3621,  0.4325, -0.5661,  ...,  2.4148, -0.8688, -0.1471],\n",
            "         [-4.1113,  0.5254, -0.3805,  ..., -0.4700, -0.2470,  0.1131],\n",
            "         ...,\n",
            "         [-1.0744, -0.3812,  0.5168,  ...,  1.7806, -0.5478,  1.1599],\n",
            "         [-0.6889, -0.4028,  0.6683,  ...,  0.4319, -0.0581,  1.6283],\n",
            "         [-1.7525,  0.4239,  1.1972,  ..., -1.0698, -0.2457,  0.6539]],\n",
            "\n",
            "        [[-0.4369, -1.2136, -0.0062,  ..., -0.9019, -0.3088, -0.2735],\n",
            "         [ 0.0945, -0.3216, -1.3299,  ...,  1.1188,  0.1962, -0.8040],\n",
            "         [-1.9672, -0.0646, -0.0974,  ..., -0.3892,  0.2757,  0.5859],\n",
            "         ...,\n",
            "         [-1.9392, -1.0152, -0.4214,  ...,  0.0208, -0.3058, -0.0306],\n",
            "         [-0.3487, -0.5770,  0.6632,  ...,  0.0467, -0.2297,  1.0076],\n",
            "         [-0.4514, -0.3683,  0.1122,  ..., -1.0596, -0.1345,  0.6279]]],\n",
            "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[ 2.1212,  1.1697,  0.8646,  ...,  0.1212, -0.2050,  0.5819],\n",
            "         [-0.5312,  0.3206, -0.4295,  ..., -1.0242,  0.9129,  0.8648],\n",
            "         [-0.4535, -0.0540, -1.9370,  ...,  0.2412, -1.2723, -0.7896],\n",
            "         ...,\n",
            "         [-0.7982,  1.7221,  0.3774,  ..., -0.5874, -0.6051,  0.1170],\n",
            "         [ 3.3027,  0.6204,  2.3785,  ..., -0.0102,  1.4376,  0.1307],\n",
            "         [ 0.4809,  0.7815,  1.3018,  ..., -0.7695,  0.9354, -0.5142]],\n",
            "\n",
            "        [[ 2.0862,  1.3344,  0.8931,  ...,  0.0081, -0.1082,  0.2361],\n",
            "         [-0.5633,  1.0850,  1.0876,  ..., -1.4068,  1.5832,  0.6973],\n",
            "         [-0.7553, -0.7781, -0.6321,  ..., -1.4367, -0.6625, -2.2956],\n",
            "         ...,\n",
            "         [-0.6062,  0.9313, -1.4788,  ..., -0.2544, -1.3365,  1.3855],\n",
            "         [ 1.8357,  2.2729,  1.4056,  ..., -0.9266,  1.2638, -2.0416],\n",
            "         [ 1.2252,  1.1558, -0.0325,  ..., -1.2231,  1.4567, -0.6558]]],\n",
            "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[ 1.3638,  0.1704,  0.9921,  ...,  0.6115, -0.3127,  0.7866],\n",
            "         [-0.8500, -0.0975, -0.1677,  ..., -0.7922,  0.6722,  0.4717],\n",
            "         [-0.8335, -0.6256, -1.3047,  ...,  0.2554, -1.2787, -0.7328],\n",
            "         ...,\n",
            "         [-1.0227,  1.1147,  0.8252,  ..., -0.7045, -0.8224, -0.3311],\n",
            "         [ 2.9936,  0.1779,  2.6037,  ...,  0.3939,  0.9621,  0.0183],\n",
            "         [ 0.3298,  0.2058,  1.8243,  ..., -0.7142,  0.9705, -0.8912]],\n",
            "\n",
            "        [[ 1.4287,  0.2652,  1.1266,  ...,  0.5573, -0.1270,  0.4770],\n",
            "         [-0.4787,  0.6598,  1.0507,  ..., -1.1229,  1.3814,  0.2712],\n",
            "         [-1.0070, -1.1059, -0.1546,  ..., -1.0679, -0.8899, -2.3725],\n",
            "         ...,\n",
            "         [-0.6359,  0.5429, -0.7647,  ..., -0.5157, -1.1739,  0.7182],\n",
            "         [ 1.5358,  1.6324,  1.5796,  ..., -0.7085,  0.9946, -2.0105],\n",
            "         [ 1.2129,  0.5200,  0.3120,  ..., -1.2789,  1.5405, -1.0986]]],\n",
            "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[ 1.8329,  0.5136,  0.8838,  ...,  0.5611, -0.7770,  0.6583],\n",
            "         [ 0.0332,  0.2084, -0.2456,  ..., -0.5686, -0.1161,  0.2646],\n",
            "         [-0.4562, -0.6521, -1.2445,  ...,  0.0452, -1.7404, -1.0707],\n",
            "         ...,\n",
            "         [-0.3604,  1.4015,  0.4476,  ..., -0.6580, -1.3174, -0.4208],\n",
            "         [ 3.3778,  0.3027,  2.4025,  ...,  0.2556,  0.2839, -0.2651],\n",
            "         [ 0.5030,  0.5061,  1.6615,  ..., -0.7401,  0.6429, -1.2872]],\n",
            "\n",
            "        [[ 1.9159,  0.4939,  1.0382,  ...,  0.3802, -0.6680,  0.4530],\n",
            "         [ 0.2738,  0.7404,  1.0078,  ..., -1.2765,  0.5409,  0.1371],\n",
            "         [-0.2373, -0.9385, -0.0629,  ..., -1.0606, -1.4398, -2.4514],\n",
            "         ...,\n",
            "         [-0.1759,  0.7211, -0.9903,  ..., -0.5794, -1.5394,  0.5719],\n",
            "         [ 2.2011,  1.5375,  1.4245,  ..., -0.9268,  0.4438, -2.0529],\n",
            "         [ 1.6747,  0.4578,  0.5539,  ..., -1.6260,  0.7112, -1.3679]]],\n",
            "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[ 2.0826,  0.3187,  0.8829,  ...,  1.4639, -0.8553,  0.3909],\n",
            "         [ 0.7327,  0.1231, -0.2062,  ...,  0.0731, -0.0257,  0.2133],\n",
            "         [-0.2313, -0.8460, -1.0028,  ...,  0.3418, -1.7614, -1.2115],\n",
            "         ...,\n",
            "         [ 0.3649,  0.9754,  0.2177,  ...,  0.1339, -1.4000, -0.5605],\n",
            "         [ 3.6814,  0.3399,  2.1687,  ...,  0.8412, -0.0907, -0.6292],\n",
            "         [ 0.8685,  0.1634,  1.1557,  ..., -0.1230,  0.4827, -1.2840]],\n",
            "\n",
            "        [[ 1.9190,  0.3372,  1.0729,  ...,  1.2998, -0.8841,  0.2596],\n",
            "         [ 0.7431,  0.5032,  0.7418,  ..., -0.6709,  0.1573,  0.0678],\n",
            "         [-0.0401, -0.9451,  0.1827,  ..., -0.8241, -1.4084, -2.5527],\n",
            "         ...,\n",
            "         [ 0.0743,  0.4501, -1.1399,  ..., -0.1220, -1.7651,  0.4362],\n",
            "         [ 2.4497,  1.6256,  1.3359,  ..., -0.5385, -0.0979, -2.1560],\n",
            "         [ 2.0584,  0.2027,  0.2520,  ..., -1.3650,  0.2406, -1.6696]]],\n",
            "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[ 1.8903e+00,  7.8142e-01,  4.6593e-01,  ...,  1.4498e+00,\n",
            "          -2.5795e-01,  1.8400e-03],\n",
            "         [ 6.6975e-01,  4.2526e-01, -6.8912e-01,  ...,  4.1005e-03,\n",
            "           5.6963e-01, -2.0805e-01],\n",
            "         [-2.7849e-01,  8.0441e-02, -1.5586e+00,  ...,  6.4592e-02,\n",
            "          -1.2236e+00, -1.4690e+00],\n",
            "         ...,\n",
            "         [ 4.4410e-01,  1.3801e+00, -2.9893e-02,  ..., -1.0947e-01,\n",
            "          -4.8953e-01, -1.0168e+00],\n",
            "         [ 3.5540e+00,  9.1490e-01,  1.5276e+00,  ...,  2.4545e-01,\n",
            "           1.7755e-01, -9.9346e-01],\n",
            "         [ 7.1290e-01,  9.0691e-01,  1.0253e+00,  ..., -3.6804e-01,\n",
            "           1.2903e+00, -1.6836e+00]],\n",
            "\n",
            "        [[ 1.7969e+00,  7.1110e-01,  4.3464e-01,  ...,  1.1058e+00,\n",
            "          -3.0595e-01, -6.0797e-02],\n",
            "         [ 7.0637e-01,  8.0949e-01,  4.2618e-01,  ..., -7.7218e-01,\n",
            "           4.6529e-01, -4.2893e-01],\n",
            "         [ 1.5811e-01, -3.3477e-01, -7.4560e-01,  ..., -9.4703e-01,\n",
            "          -9.6643e-01, -2.7382e+00],\n",
            "         ...,\n",
            "         [ 4.3866e-03,  8.6066e-01, -1.4198e+00,  ..., -6.2818e-01,\n",
            "          -1.1201e+00, -5.5342e-03],\n",
            "         [ 2.4569e+00,  1.9314e+00,  6.0959e-01,  ..., -9.5761e-01,\n",
            "           4.1346e-01, -2.6188e+00],\n",
            "         [ 1.9599e+00,  6.3096e-01, -2.6633e-01,  ..., -1.5780e+00,\n",
            "           5.6452e-01, -1.9699e+00]]], device='cuda:0',\n",
            "       grad_fn=<NativeLayerNormBackward>)\n",
            "tensor([[[ 1.7451,  0.9585,  0.3183,  ...,  1.0913, -0.0285,  0.1724],\n",
            "         [ 0.2436,  0.3213, -0.8159,  ..., -0.3665,  0.4447,  0.1239],\n",
            "         [-0.2825, -0.2320, -1.6412,  ..., -0.5117, -0.8105, -1.3676],\n",
            "         ...,\n",
            "         [ 0.6738,  1.4372, -0.2095,  ..., -0.4549,  0.0409, -0.8548],\n",
            "         [ 3.2599,  0.9927,  1.1942,  ..., -0.1504,  0.4185, -0.9661],\n",
            "         [ 0.7660,  1.0472,  1.1043,  ..., -0.5707,  1.5210, -1.8038]],\n",
            "\n",
            "        [[ 1.5306,  0.8220,  0.1089,  ...,  0.8350, -0.0041,  0.1855],\n",
            "         [ 0.4557,  0.8121, -0.0986,  ..., -0.7992,  0.6720, -0.0352],\n",
            "         [-0.0379, -0.4244, -1.1305,  ..., -1.1617, -0.9353, -2.2097],\n",
            "         ...,\n",
            "         [ 0.0495,  0.8025, -1.6270,  ..., -1.2071, -0.5432,  0.1694],\n",
            "         [ 2.3149,  2.0711, -0.0246,  ..., -1.0332,  0.7711, -2.5072],\n",
            "         [ 1.6486,  0.7184, -0.2075,  ..., -1.5808,  0.6751, -1.9357]]],\n",
            "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n",
            "torch.Size([2, 7, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}